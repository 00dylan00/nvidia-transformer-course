{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59169381",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465cdd4e",
   "metadata": {
    "id": "tiIOhb7iVC3J"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c6e1d9",
   "metadata": {
    "id": "PucJwfbhVC3L"
   },
   "source": [
    "## Task Description\n",
    "\n",
    "- Given a context and a natural language query, we want to generate an answer for the query\n",
    "- Depending on how the answer is generated, the task can be broadly divided into two types:\n",
    "    1. Extractive Question Answering\n",
    "    2. <b>Generative Question Answering</b>\n",
    "\n",
    "### Generative Question-Answering with S2S and GPT-like models\n",
    "\n",
    "Given a question and a context, both in natural language, generate an answer for the question. Unlike the BERT-like models, there is no constraint that the answer should be a span within the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da43b269",
   "metadata": {
    "id": "_xQBtr0KVC3M"
   },
   "outputs": [],
   "source": [
    "BRANCH = 'main'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f1464",
   "metadata": {
    "id": "fof5-57iVC3N"
   },
   "source": [
    "# Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c03db017",
   "metadata": {
    "id": "KqKD-wReVC3O"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import gc\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from nemo.collections.nlp.models.question_answering.qa_gpt_model import GPTQAModel\n",
    "from nemo.collections.nlp.models.question_answering.qa_s2s_model import S2SQAModel\n",
    "\n",
    "gc.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56f4309b",
   "metadata": {
    "id": "xhPr9Jf_VC3O"
   },
   "outputs": [],
   "source": [
    "# set the following paths\n",
    "DATA_DIR = \"data\" # directory for storing datasets\n",
    "WORK_DIR = \"work_dir\" # directory for storing trained models, logs, additionally downloaded scripts\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(WORK_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959ba02",
   "metadata": {
    "id": "dWymW8e0VC3O"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16d5d5",
   "metadata": {
    "id": "0YhKTkuXVC3P"
   },
   "source": [
    "The model is defined in a config file which declares multiple important sections:\n",
    "- **model**: All arguments that will relate to the Model - language model, span prediction, optimizer and schedulers, datasets and any other related information\n",
    "- **trainer**: Any argument to be passed to PyTorch Lightning\n",
    "- **exp_manager**: All arguments used for setting up the experiment manager - target directory, name, logger information\n",
    "\n",
    "We will download the default config file provided at `NeMo/examples/nlp/question_answering/conf/qa_conf.yaml` and edit necessary values for training different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d32d8b49",
   "metadata": {
    "id": "WOIWJqQ0VC3P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file already exists\n"
     ]
    }
   ],
   "source": [
    "# download the model's default configuration file \n",
    "config_dir = WORK_DIR + '/conf/'\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "if not os.path.exists(config_dir + \"qa_conf.yaml\"):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/question_answering/conf/qa_conf.yaml', config_dir)\n",
    "else:\n",
    "    print ('config file already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ca2cfee",
   "metadata": {
    "id": "cvD-gv-FVC3P",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work_dir/conf/qa_conf.yaml\n",
      "Default Config - \n",
      "\n",
      "pretrained_model: null\n",
      "do_training: true\n",
      "trainer:\n",
      "  devices:\n",
      "  - 0\n",
      "  num_nodes: 1\n",
      "  max_epochs: 3\n",
      "  max_steps: -1\n",
      "  accumulate_grad_batches: 1\n",
      "  gradient_clip_val: 1.0\n",
      "  precision: 16\n",
      "  accelerator: gpu\n",
      "  log_every_n_steps: 5\n",
      "  val_check_interval: 1.0\n",
      "  num_sanity_val_steps: 0\n",
      "  enable_checkpointing: false\n",
      "  logger: false\n",
      "  strategy: ddp\n",
      "model:\n",
      "  tensor_model_parallel_size: 1\n",
      "  nemo_path: null\n",
      "  library: huggingface\n",
      "  save_model: false\n",
      "  tokens_to_generate: 32\n",
      "  dataset:\n",
      "    version_2_with_negative: true\n",
      "    doc_stride: 128\n",
      "    max_query_length: 64\n",
      "    max_seq_length: 512\n",
      "    max_answer_length: 30\n",
      "    use_cache: false\n",
      "    do_lower_case: true\n",
      "    check_if_answer_in_context: true\n",
      "    keep_doc_spans: all\n",
      "    null_score_diff_threshold: 0.0\n",
      "    n_best_size: 20\n",
      "    num_workers: 1\n",
      "    pin_memory: false\n",
      "    drop_last: false\n",
      "  train_ds:\n",
      "    file: null\n",
      "    batch_size: 24\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    num_workers: ${model.dataset.num_workers}\n",
      "    drop_last: ${model.dataset.drop_last}\n",
      "    pin_memory: ${model.dataset.pin_memory}\n",
      "  validation_ds:\n",
      "    file: null\n",
      "    batch_size: 24\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: ${model.dataset.num_workers}\n",
      "    drop_last: ${model.dataset.drop_last}\n",
      "    pin_memory: ${model.dataset.pin_memory}\n",
      "  test_ds:\n",
      "    file: null\n",
      "    batch_size: 24\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: ${model.dataset.num_workers}\n",
      "    drop_last: ${model.dataset.drop_last}\n",
      "    pin_memory: ${model.dataset.pin_memory}\n",
      "  language_model:\n",
      "    pretrained_model_name: bert-base-uncased\n",
      "    lm_checkpoint: null\n",
      "    config_file: null\n",
      "    config: null\n",
      "  token_classifier:\n",
      "    num_layers: 1\n",
      "    dropout: 0.0\n",
      "    num_classes: 2\n",
      "    activation: relu\n",
      "    log_softmax: false\n",
      "    use_transformer_init: true\n",
      "  tokenizer:\n",
      "    tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "    vocab_file: null\n",
      "    tokenizer_model: null\n",
      "    special_tokens: null\n",
      "  optim:\n",
      "    name: adamw\n",
      "    lr: 5.0e-05\n",
      "    betas:\n",
      "    - 0.9\n",
      "    - 0.999\n",
      "    weight_decay: 0.0\n",
      "    sched:\n",
      "      name: SquareRootAnnealing\n",
      "      warmup_steps: null\n",
      "      warmup_ratio: 0.0\n",
      "      last_epoch: -1\n",
      "      monitor: val_loss\n",
      "      reduce_on_plateau: false\n",
      "exp_manager:\n",
      "  exp_dir: null\n",
      "  name: QnA\n",
      "  create_wandb_logger: false\n",
      "  wandb_logger_kwargs:\n",
      "    name: ???\n",
      "    project: QnA\n",
      "  create_tensorboard_logger: true\n",
      "  create_checkpoint_callback: true\n",
      "  resume_if_exists: false\n",
      "  resume_ignore_no_checkpoint: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this will print the entire default config of the model\n",
    "config_path = f'{WORK_DIR}/conf/qa_conf.yaml'\n",
    "print(config_path)\n",
    "config = OmegaConf.load(config_path)\n",
    "print(\"Default Config - \\n\")\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd9e0a6",
   "metadata": {
    "id": "E08e-ItPVC3P"
   },
   "source": [
    "# Training and testing models on SQuAD v2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35d6a1",
   "metadata": {
    "id": "xn022MsKVC3Q"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65efc963",
   "metadata": {
    "id": "c356CGL1VC3Q"
   },
   "source": [
    "For this example, we are going to download the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) dataset to showcase how to do training and inference. There are two datasets, SQuAD1.0 and SQuAD2.0. SQuAD 1.1, the previous version of the SQuAD dataset, contains 100,000+ question-answer pairs on 500+ articles. SQuAD2.0 dataset combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2cb91f",
   "metadata": {
    "id": "nprGkyvRVC3Q"
   },
   "source": [
    "We have prepared the data directory \"squad\" with the following four files for training and evaluation: \n",
    "\n",
    "```\n",
    "squad  \n",
    "│\n",
    "└───v1.1\n",
    "│   │ -  train-v1.1.json\n",
    "│   │ -  dev-v1.1.json\n",
    "│\n",
    "└───v2.0\n",
    "    │ -  train-v2.0.json\n",
    "    │ -  dev-v2.0.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74500966",
   "metadata": {
    "id": "GX0KWQXKVC3Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/squad:\n",
      "v1.1  v2.0\n",
      "\n",
      "data/squad/v1.1:\n",
      "dev-v1.1.json  train-v1.1.json\n",
      "\n",
      "data/squad/v2.0:\n",
      "dev-v2.0.json  train-v2.0.json\n"
     ]
    }
   ],
   "source": [
    "!ls -LR {DATA_DIR}/squad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c642dde",
   "metadata": {
    "id": "RFVcvseOVC3R"
   },
   "source": [
    "## Set dataset config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1887338b",
   "metadata": {
    "id": "Grb0EeRqVC3R"
   },
   "outputs": [],
   "source": [
    "# if True, model will load features from cache if file is present, or\n",
    "# create features and dump to cache file if not already present\n",
    "config.model.dataset.use_cache = False\n",
    "\n",
    "# indicates whether the dataset has unanswerable questions\n",
    "config.model.dataset.version_2_with_negative = True\n",
    "\n",
    "# indicates whether the dataset is of extractive nature or not\n",
    "# if True, context spans/chunks that do not contain answer are treated as unanswerable \n",
    "config.model.dataset.check_if_answer_in_context = True\n",
    "\n",
    "# set file paths for train, validation, and test datasets\n",
    "config.model.train_ds.file = f\"{DATA_DIR}/squad/v2.0/train-v2.0.json\"\n",
    "config.model.validation_ds.file = f\"{DATA_DIR}/squad/v2.0/dev-v2.0.json\"\n",
    "config.model.test_ds.file = f\"{DATA_DIR}/squad/v2.0/dev-v2.0.json\"\n",
    "\n",
    "# set batch sizes for train, validation, and test datasets\n",
    "config.model.train_ds.batch_size = 8\n",
    "config.model.validation_ds.batch_size = 8\n",
    "config.model.test_ds.batch_size = 8\n",
    "\n",
    "# set number of samples to be used from dataset. setting to -1 uses entire dataset\n",
    "config.model.train_ds.num_samples = 5000\n",
    "config.model.validation_ds.num_samples = 1000\n",
    "config.model.test_ds.num_samples = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c9cd3",
   "metadata": {
    "id": "rFWF41VwVC3R"
   },
   "source": [
    "## Set trainer config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2f5c347",
   "metadata": {
    "id": "42yif-GIVC3R"
   },
   "outputs": [],
   "source": [
    "config.trainer.max_epochs = 1\n",
    "config.trainer.max_steps = -1 # takes precedence over max_epochs\n",
    "config.trainer.precision = 16\n",
    "config.trainer.devices = [0] # 0 for CPU, or list of the GPUs to use [0] this tutorial does not support multiple GPUs. If needed please use NeMo/examples/nlp/question_answering/question_answering.py\n",
    "config.trainer.accelerator = \"gpu\"\n",
    "config.trainer.strategy=\"auto\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dda6e0",
   "metadata": {
    "id": "EDQzMBlbVC3R"
   },
   "source": [
    "## Set experiment manager config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "110efc69",
   "metadata": {
    "id": "pxY4rnJBVC3R"
   },
   "outputs": [],
   "source": [
    "# config.exp_manager.exp_dir = WORK_DIR\n",
    "# config.exp_manager.name = \"QA-SQuAD2\"\n",
    "# config.exp_manager.create_wandb_logger=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6637164",
   "metadata": {
    "id": "zyh0SNiyVC3S"
   },
   "source": [
    "## S2S BART model for SQuAD v2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be37e40c",
   "metadata": {
    "id": "Sy9IYgVYVC3S"
   },
   "source": [
    "### Set model config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f45d88b3",
   "metadata": {
    "id": "PKNmHKV5VC3S"
   },
   "outputs": [],
   "source": [
    "# set language model and tokenizer to be used\n",
    "# tokenizer is derived from model if a tokenizer name is not provided\n",
    "config.model.language_model.pretrained_model_name = \"facebook/bart-base\"\n",
    "config.model.tokenizer.tokenizer_name = \"facebook/bart-base\"\n",
    "\n",
    "# path where model will be saved\n",
    "config.model.nemo_path = f\"{WORK_DIR}/checkpoints/bart_squad_v2_0.nemo\"\n",
    "\n",
    "config.exp_manager.create_checkpoint_callback = True\n",
    "\n",
    "config.model.optim.lr = 5e-5\n",
    "\n",
    "#remove vocab_file from gpt model\n",
    "config.model.tokenizer.vocab_file = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bbc243",
   "metadata": {
    "id": "S_0glS4yVC3S"
   },
   "source": [
    "### Create trainer and initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4ad52",
   "metadata": {
    "id": "8jWyHY1oVC3S",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-11 15:00:20 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: facebook/bart-base, vocab_file: None, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-07-11 15:00:20 modelPT:244] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 839.2727272727273\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1895\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 677.5487804878048\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1782\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 828.0972222222222\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 2132\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 540.0\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1423\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 756.71875\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1747\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 732.4418604651163\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 3076\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 574.5454545454545\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1324\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 625.9256756756756\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 2073\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 871.4354838709677\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1586\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 607.2692307692307\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1812\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 839.4556962025316\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1881\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 513.8523489932886\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1416\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 547.0472440944882\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1556\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 541.44\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1330\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 818.8513513513514\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 2121\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 450.36\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1103\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 585.64\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1648\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 472.53846153846155\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1229\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 449.9166666666667\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 866\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 486.84415584415586\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 2571\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 457.5\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1281\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 475.4761904761905\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 747\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 587.7391304347826\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1496\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 588.9130434782609\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1602\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 566.8888888888889\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1296\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 564.6666666666666\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1307\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 528.7543859649123\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1196\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 672.8275862068965\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1847\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 493.0\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1261\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 362.6216216216216\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1613\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 427.24\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 982\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 475.0967741935484\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1149\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 634.3255813953489\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 2173\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 565.8518518518518\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1435\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 559.3793103448276\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1492\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 586.304347826087\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1346\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 481.969696969697\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1308\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 453.44444444444446\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1590\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 394.0285714285714\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1619\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 513.0975609756098\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:107] max no. of chars in doc: 1384\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 516.0588235294117\n",
      "[NeMo I 2024-07-11 15:00:20 qa_processing:106] mean no. of chars in doc: 516.0588235294117\n"
     ]
    }
   ],
   "source": [
    "# uncomment below line and run if you get an error while initializing tokenizer on Colab (reference: https://github.com/huggingface/transformers/issues/8690)\n",
    "# !rm -r /root/.cache/huggingface/\n",
    "\n",
    "trainer = pl.Trainer(**config.trainer)\n",
    "model = S2SQAModel(config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836ff6e",
   "metadata": {
    "id": "xg-j39b4VC3S"
   },
   "source": [
    "### Train, test, and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b38b6",
   "metadata": {
    "id": "ocsf0EBDVC3S",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)\n",
    "trainer.test(model)\n",
    "\n",
    "model.save_to(config.model.nemo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b5e5c",
   "metadata": {
    "id": "Vs3pl0VMVC3S"
   },
   "source": [
    "### Load the saved model and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d5920",
   "metadata": {
    "id": "NoW6_GO_VC3S",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = S2SQAModel.restore_from(config.model.nemo_path)\n",
    "\n",
    "eval_device = [config.trainer.devices[0]] if isinstance(config.trainer.devices, list) else 1\n",
    "model.trainer = pl.Trainer(\n",
    "    devices=eval_device,\n",
    "    accelerator=config.trainer.accelerator,\n",
    "    precision=16,\n",
    "    logger=False,\n",
    ")\n",
    "\n",
    "all_preds, all_nbest = model.inference(\n",
    "    config.model.test_ds.file,\n",
    "#     output_prediction_file=output_prediction_file,\n",
    "#     output_nbest_file=output_nbest_file,\n",
    "    num_samples=10, # setting to -1 will use all samples for inference\n",
    ")\n",
    "\n",
    "for question_id in all_preds:\n",
    "    print(all_preds[question_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619330ef",
   "metadata": {
    "id": "a7-iInbPVC3S"
   },
   "source": [
    "## GPT2 model for SQuAD v2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5032e",
   "metadata": {
    "id": "VaIC0l2aVC3S"
   },
   "source": [
    "### Exercise # 1 - Set model config values\n",
    "\n",
    "* Modify the `<FIXME>` to use the `gpt2` pre-trained model and tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c77de",
   "metadata": {
    "id": "5j6SVk6fVC3S"
   },
   "outputs": [],
   "source": [
    "# set language model and tokenizer to be used\n",
    "# tokenizer is derived from model if a tokenizer name is not provided\n",
    "config.model.language_model.pretrained_model_name = \"gpt2\"\n",
    "config.model.tokenizer.tokenizer_name = \"gpt2\"\n",
    "\n",
    "# path where model will be saved\n",
    "config.model.nemo_path = f\"{WORK_DIR}/checkpoints/gpt2_squad_v2_0.nemo\"\n",
    "\n",
    "config.exp_manager.create_checkpoint_callback = True\n",
    "\n",
    "config.model.optim.lr = 1e-4"
   ]
  },
  {
   "cell_type": "raw",
   "id": "387fa5cd",
   "metadata": {
    "id": "5j6SVk6fVC3S"
   },
   "source": [
    "# set language model and tokenizer to be used\n",
    "# tokenizer is derived from model if a tokenizer name is not provided\n",
    "config.model.language_model.pretrained_model_name = \"gpt2\"\n",
    "config.model.tokenizer.tokenizer_name = \"gpt2\"\n",
    "\n",
    "# path where model will be saved\n",
    "config.model.nemo_path = f\"{WORK_DIR}/checkpoints/gpt2_squad_v2_0.nemo\"\n",
    "\n",
    "config.exp_manager.create_checkpoint_callback = True\n",
    "\n",
    "config.model.optim.lr = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f6394",
   "metadata": {},
   "source": [
    "click ... to show solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d752aaae",
   "metadata": {
    "id": "rWhhEuvzVC3S"
   },
   "source": [
    "### Create trainer and initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287a30b4",
   "metadata": {
    "id": "vBtP3ukDVC3S",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(**config.trainer)\n",
    "model = GPTQAModel(config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f95876",
   "metadata": {
    "id": "EApFrJh8VC3T"
   },
   "source": [
    "### Exercise # 2 - Train, test, and save the model\n",
    "\n",
    "* Modify the `<FIXME>` to train, test, and save the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bfa560",
   "metadata": {
    "id": "zYo2JDdOVC3T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)\n",
    "trainer.test(model)\n",
    "\n",
    "model.save_to(config.model.nemo_path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3dde0d0e",
   "metadata": {
    "id": "zYo2JDdOVC3T"
   },
   "source": [
    "trainer.fit(model)\n",
    "trainer.test(model)\n",
    "\n",
    "model.save_to(config.model.nemo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438378c1",
   "metadata": {},
   "source": [
    "click ... to show solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f646c7",
   "metadata": {
    "id": "6aNEt06fVC3T"
   },
   "source": [
    "### Exercise # 3 - Load the saved model and run inference\n",
    "\n",
    "* Modify the `<FIXME>` to run inference from a saved model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dec762",
   "metadata": {
    "id": "ioLT4DVbVC3T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = GPTQAModel.restore_from(config.model.nemo_path)\n",
    "\n",
    "eval_device = [config.trainer.devices[0]] if isinstance(config.trainer.devices, list) else 1\n",
    "model.trainer = pl.Trainer(\n",
    "    devices=eval_device,\n",
    "    accelerator=config.trainer.accelerator,\n",
    "    precision=16,\n",
    "    logger=False,\n",
    ")\n",
    "\n",
    "all_preds, all_nbest = model.inference(\n",
    "    config.model.test_ds.file,\n",
    "    num_samples=10, # setting to -1 will use all samples for inference\n",
    ")\n",
    "\n",
    "for question_id in all_preds:\n",
    "    print(all_preds[question_id])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2a2eadc",
   "metadata": {
    "id": "ioLT4DVbVC3T"
   },
   "source": [
    "model = GPTQAModel.restore_from(config.model.nemo_path)\n",
    "\n",
    "eval_device = [config.trainer.devices[0]] if isinstance(config.trainer.devices, list) else 1\n",
    "model.trainer = pl.Trainer(\n",
    "    devices=eval_device,\n",
    "    accelerator=config.trainer.accelerator,\n",
    "    precision=16,\n",
    "    logger=False,\n",
    ")\n",
    "\n",
    "all_preds, all_nbest = model.inference(\n",
    "    config.model.test_ds.file,\n",
    "    num_samples=10, # setting to -1 will use all samples for inference\n",
    ")\n",
    "\n",
    "for question_id in all_preds:\n",
    "    print(all_preds[question_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7807246d",
   "metadata": {},
   "source": [
    "click ... to show solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33d748",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e753a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
